{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3opHLYJRHCR"
   },
   "source": [
    "# Automatic Speech Recognition with Transformer\n",
    "\n",
    "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
    "**Date created:** 2021/01/13<br>\n",
    "**Last modified:** 2021/01/13<br>\n",
    "**Description:** Training a sequence-to-sequence Transformer for automatic speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNu5BNVtRHCW"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n",
    "ASR can be treated as a sequence-to-sequence problem, where the\n",
    "audio can be represented as a sequence of feature vectors\n",
    "and the text as a sequence of characters, words, or subword tokens.\n",
    "\n",
    "For this demonstration, we will use the LJSpeech dataset from the\n",
    "[LibriVox](https://librivox.org/) project. It consists of short\n",
    "audio clips of a single speaker reading passages from 7 non-fiction books.\n",
    "Our model will be similar to the original Transformer (both encoder and decoder)\n",
    "as proposed in the paper, \"Attention is All You Need\".\n",
    "\n",
    "\n",
    "**References:**\n",
    "\n",
    "- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/abs/1904.13377)\n",
    "- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n",
    "- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "DaN6AecBRHCX",
    "outputId": "63da7ee3-6437-49b4-b800-e20b163ac42b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # ou qualquer número de 0 a 3\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import datetime\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "#print(tf.__version__)\n",
    "#print(keras.__version__)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Lista todos os dispositivos físicos do tipo 'GPU'\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(len(devices))  # Se o resultado for maior que 0, uma GPU está sendo usada\n",
    "\n",
    "# Verifica se o TensorFlow foi construído com suporte a CUDA\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjOYcgmpRHCZ"
   },
   "source": [
    "## Define the Transformer Input Layer\n",
    "\n",
    "When processing past target tokens for the decoder, we compute the sum of\n",
    "position embeddings and token embeddings.\n",
    "\n",
    "When processing audio features, we apply convolutional layers to downsample\n",
    "them (via convolution strides) and process local relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Irr1T-O2RHCZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.emb = keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class SpeechFeatureEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ab14XhPRHCa"
   },
   "source": [
    "## Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SpEtRUFDRHCb"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfsTgZ6vRHCb"
   },
   "source": [
    "## Transformer Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LRksSL9oRHCc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
    "        return ffn_out_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d55jG_L0RHCc"
   },
   "source": [
    "## Complete the Transformer model\n",
    "\n",
    "Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n",
    "During training, we give the decoder the target character sequence shifted to the left\n",
    "as input. During inference, the decoder uses its own past predictions to predict the\n",
    "next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OgptmFnFRHCd"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=100,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=4,\n",
    "        num_layers_dec=1,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        \n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "        \n",
    "        ####################### Congela todas as camadas decodificadoras ################################\n",
    "\n",
    "        #for i in range(num_layers_dec):\n",
    "        #    dec_layer = TransformerDecoder(num_hid, num_head, num_feed_forward)\n",
    "        #    dec_layer.trainable = False  # Congelar a camada\n",
    "        #    setattr(self, f\"dec_layer_{i}\", dec_layer)\n",
    "            \n",
    "        ########################Congela apenas uma camada conforme o indice #############################\n",
    "        #\n",
    "        # Supondo que tenha 3 camadas de decodificador e queira congelar apenas a segunda camada\n",
    "        '''\n",
    "        for i in range(num_layers_dec):\n",
    "            dec_layer = TransformerDecoder(num_hid, num_head, num_feed_forward)\n",
    "            if i == 1:  # Se for a segunda camada (índice 1)\n",
    "                dec_layer.trainable = False  # Congelar a camada\n",
    "            setattr(self, f\"dec_layer_{i}\", dec_layer)\n",
    "        '''\n",
    "        #################################################################################################\n",
    "        \n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        x = self.encoder(source)\n",
    "        y = self.decode(x, target)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "            loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "            loss = loss_object(one_hot, preds, sample_weight=mask)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "    \n",
    "    def test_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "\n",
    "        # Calculando a perda\n",
    "        loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "        loss = loss_object(one_hot, preds, sample_weight=mask)\n",
    "\n",
    "        # Convertendo previsões em índices de palavras\n",
    "        pred_words = tf.argmax(preds, axis=-1)\n",
    "        target_words = dec_target\n",
    "\n",
    "        # Calculando a taxa de acerto de palavras\n",
    "        word_accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.cast(pred_words, tf.int64), tf.cast(target_words, tf.int64)), tf.float32))\n",
    "\n",
    "        # Atualizando a métrica de perda\n",
    "        self.loss_metric.update_state(loss)\n",
    "\n",
    "        return {\"word_accuracy\": word_accuracy, \"loss\": self.loss_metric.result()}\n",
    "\n",
    "    \n",
    "    ######### Original #############\n",
    "    '''\n",
    "    def test_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "        loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss = loss_object(one_hot, preds, sample_weight=mask)\n",
    "\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "    '''\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liuKciSRRHCd"
   },
   "source": [
    "## Download the dataset\n",
    "\n",
    "Note: This requires ~3.6 GB of disk space and\n",
    "takes ~5 minutes for the extraction of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "au3uVjM1RHCd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'audio': '.\\\\datasets\\\\uaspeech\\\\wavs\\\\CF02\\\\CF02_B1_C10_M3.wav', 'text': 'Line'}, {'audio': '.\\\\datasets\\\\uaspeech\\\\wavs\\\\CF02\\\\CF02_B1_C10_M4.wav', 'text': 'Line'}, {'audio': '.\\\\datasets\\\\uaspeech\\\\wavs\\\\CF02\\\\CF02_B1_C10_M5.wav', 'text': 'Line'}, {'audio': '.\\\\datasets\\\\uaspeech\\\\wavs\\\\CF02\\\\CF02_B1_C10_M6.wav', 'text': 'Line'}]\n",
      "[{'audio': '.\\\\datasets\\\\uaspeech\\\\test\\\\CM06\\\\CM06_B1_C10_M3.wav', 'text': 'Line'}, {'audio': '.\\\\datasets\\\\uaspeech\\\\test\\\\CM06\\\\CM06_B1_C10_M4.wav', 'text': 'Line'}, {'audio': '.\\\\datasets\\\\uaspeech\\\\test\\\\CM06\\\\CM06_B1_C10_M5.wav', 'text': 'Line'}, {'audio': '.\\\\datasets\\\\uaspeech\\\\test\\\\CM06\\\\CM06_B1_C10_M6.wav', 'text': 'Line'}]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "keras.utils.get_file(\n",
    "    os.path.join(os.getcwd(), \"data.tar.gz\"),\n",
    "    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n",
    "    extract=True,\n",
    "    archive_format=\"tar\",\n",
    "    cache_dir=\".\",\n",
    ")\n",
    "'''\n",
    "\n",
    "#saveto = \".\\\\datasets\\\\LJSpeech-1.1\"\n",
    "saveto = '.\\\\datasets\\\\uaspeech\\\\wavs'\n",
    "wavs = glob(\"{}\\\\**\\\\*.wav\".format(saveto), recursive=True)\n",
    "id_to_text = {}\n",
    "with open(os.path.join(saveto, \".\\\\transcript\\metadata.csv\"), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        id = line.strip().split(\"|\")[0]\n",
    "        text = line.strip().split(\"|\")[1]\n",
    "        id_to_text[id] = text\n",
    "\n",
    "saveto_test = '.\\\\datasets\\\\uaspeech\\\\test'\n",
    "wavs_test = glob(\"{}\\\\**\\\\*.wav\".format(saveto_test), recursive=True)\n",
    "id_to_text_test = {}\n",
    "with open(os.path.join(saveto_test, \".\\\\transcript\\metadata.csv\"), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        id_test = line.strip().split(\"|\")[0]\n",
    "        text_test = line.strip().split(\"|\")[1]\n",
    "        id_to_text_test[id_test] = text_test        \n",
    "        \n",
    "\n",
    "def get_data(wavs, id_to_text, maxlen=50):\n",
    "    \"\"\"returns mapping of audio paths and transcription texts\"\"\"\n",
    "    data = []\n",
    "    for w in wavs:\n",
    "        id = w.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        if len(id_to_text[id]) < maxlen:\n",
    "            data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
    "    return data\n",
    "\n",
    "def get_data_test(wavs_test, id_to_text_test, maxlen=50):\n",
    "    \"\"\"returns mapping of audio paths and transcription texts\"\"\"\n",
    "    data_test = []\n",
    "    for w in wavs_test:\n",
    "        id_test = w.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        if len(id_to_text_test[id_test]) < maxlen:\n",
    "            data_test.append({\"audio\": w, \"text\": id_to_text_test[id_test]})\n",
    "    return data_test\n",
    "\n",
    "print(get_data(wavs, id_to_text, maxlen=50)[1:5])\n",
    "print(get_data_test(wavs_test, id_to_text_test, maxlen=50)[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eTa9km4RHCe"
   },
   "source": [
    "## Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2mxWQLOARHCe",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab\n",
    "\n",
    "\n",
    "max_target_len = 200  # all transcripts in out data are < 200 characters\n",
    "data = get_data(wavs, id_to_text, max_target_len)\n",
    "data_test = get_data_test(wavs_test, id_to_text_test, max_target_len)\n",
    "vectorizer = VectorizeChar(max_target_len)\n",
    "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "\n",
    "\n",
    "def create_text_ds(data):\n",
    "    texts = [_[\"text\"] for _ in data]\n",
    "    text_ds = [vectorizer(t) for t in texts]\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
    "    return text_ds\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    # spectrogram using stft\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
    "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
    "    # normalisation\n",
    "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
    "    x = (x - means) / stddevs\n",
    "    audio_len = tf.shape(x)[0]\n",
    "    # padding to 10 seconds\n",
    "    pad_len = 2754\n",
    "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
    "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_audio_ds(data):\n",
    "    flist = [_[\"audio\"] for _ in data]\n",
    "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
    "    audio_ds = audio_ds.map(path_to_audio, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return audio_ds\n",
    "\n",
    "\n",
    "def create_tf_dataset(data, bs=64):\n",
    "    audio_ds = create_audio_ds(data)\n",
    "    text_ds = create_text_ds(data)\n",
    "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
    "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
    "    ds = ds.batch(bs)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split = int(len(data) * 0.99)\n",
    "#train_data = data[:split]\n",
    "train_data = data\n",
    "#test_data = data[split:]\n",
    "test_data = data_test\n",
    "ds = create_tf_dataset(train_data, bs=64)\n",
    "val_ds = create_tf_dataset(test_data, bs=4) #orig. = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: {source: (None, None, 129), target: (None, 200)}, types: {source: tf.float32, target: tf.int32}>\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: {source: (None, None, 129), target: (None, 200)}, types: {source: tf.float32, target: tf.int32}>\n"
     ]
    }
   ],
   "source": [
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szBaODmsRHCg"
   },
   "source": [
    "## Callbacks to display predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1L-YvvMxRHCg"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
    "    ):\n",
    "        \"\"\"Displays a batch of outputs after every epoch\n",
    "\n",
    "        Args:\n",
    "            batch: A test batch containing the keys \"source\" and \"target\"\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 != 0:\n",
    "            return\n",
    "        source = self.batch[\"source\"]\n",
    "        target = self.batch[\"target\"].numpy()\n",
    "        bs = tf.shape(source)[0]\n",
    "        preds = self.model.generate(source, self.target_start_token_idx)\n",
    "        preds = preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += self.idx_to_char[idx]\n",
    "                if idx == self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:     {target_text.replace('-','')}\")\n",
    "            print(f\"prediction: {prediction}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JfOL_oARHCg"
   },
   "source": [
    "## Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "D-dRLFkNRHCh"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_lr=0.00001,\n",
    "        lr_after_warmup=0.001,\n",
    "        final_lr=0.00001,\n",
    "        warmup_epochs=15,\n",
    "        decay_epochs=85,\n",
    "        steps_per_epoch=203,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_after_warmup = lr_after_warmup\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def calculate_lr(self, epoch):\n",
    "        \"\"\"linear warm up - linear decay\"\"\"\n",
    "        warmup_lr = (\n",
    "            self.init_lr\n",
    "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
    "        )\n",
    "        decay_lr = tf.math.maximum(\n",
    "            self.final_lr,\n",
    "            self.lr_after_warmup\n",
    "            - (epoch - self.warmup_epochs)\n",
    "            * (self.lr_after_warmup - self.final_lr)\n",
    "            / self.decay_epochs,\n",
    "        )\n",
    "        return tf.math.minimum(warmup_lr, decay_lr)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = step // self.steps_per_epoch\n",
    "        epoch = tf.cast(epoch, \"float32\")\n",
    "        return self.calculate_lr(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ne33ZwMORHCh"
   },
   "source": [
    "## Create & train the end-to-end model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva e carrega o modelo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# def create_callbacks(display_cb, filepath):\n",
    "    # Função para salvar o melhor modelo baseado na perda de validação\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "\n",
    "    # Adicione model_checkpoint_callback à lista de callbacks.\n",
    "    callbacks_list = [display_cb, model_checkpoint_callback]\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(ckpt, manager):\n",
    "    # Função para retomar o treinamento de onde parou\n",
    "    ckpt.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restaurado de {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Inicializando do zero.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bby6svtcRHCh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech_feature_embedding_2 True\n",
      "transformer_encoder_8 True\n",
      "transformer_encoder_9 True\n",
      "transformer_encoder_10 False\n",
      "transformer_encoder_11 False\n",
      "Inicializando do zero.\n",
      "Epoch 1/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0926\n",
      "Epoch 00001: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 889s 879ms/step - loss: 0.0926 - val_word_accuracy: 0.0101 - val_loss: 0.0809\n",
      "Epoch 2/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0698\n",
      "Epoch 00002: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 882s 878ms/step - loss: 0.0698 - val_word_accuracy: 0.0151 - val_loss: 0.0595\n",
      "Epoch 3/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0509\n",
      "Epoch 00003: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 882s 878ms/step - loss: 0.0509 - val_word_accuracy: 0.0251 - val_loss: 0.0375\n",
      "Epoch 4/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0316\n",
      "Epoch 00004: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 873s 869ms/step - loss: 0.0316 - val_word_accuracy: 0.0251 - val_loss: 0.0264\n",
      "Epoch 5/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0233\n",
      "Epoch 00005: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 880s 875ms/step - loss: 0.0233 - val_word_accuracy: 0.0235 - val_loss: 0.0203\n",
      "Epoch 6/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0190\n",
      "Epoch 00006: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 882s 877ms/step - loss: 0.0190 - val_word_accuracy: 0.0302 - val_loss: 0.0194\n",
      "Epoch 7/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0165\n",
      "Epoch 00007: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 880s 876ms/step - loss: 0.0165 - val_word_accuracy: 0.0302 - val_loss: 0.0179\n",
      "Epoch 8/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0143\n",
      "Epoch 00008: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0143 - val_word_accuracy: 0.0302 - val_loss: 0.0160\n",
      "Epoch 9/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0143\n",
      "Epoch 00009: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 881s 876ms/step - loss: 0.0143 - val_word_accuracy: 0.0302 - val_loss: 0.0249\n",
      "Epoch 10/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0136\n",
      "Epoch 00010: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 881s 877ms/step - loss: 0.0136 - val_word_accuracy: 0.0302 - val_loss: 0.0155\n",
      "Epoch 11/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0168\n",
      "Epoch 00011: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0168 - val_word_accuracy: 0.0302 - val_loss: 0.0168\n",
      "Epoch 12/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0141\n",
      "Epoch 00012: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 880s 875ms/step - loss: 0.0141 - val_word_accuracy: 0.0251 - val_loss: 0.0145\n",
      "Epoch 13/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0136\n",
      "Epoch 00013: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 882s 877ms/step - loss: 0.0136 - val_word_accuracy: 0.0302 - val_loss: 0.0131\n",
      "Epoch 14/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0130\n",
      "Epoch 00014: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 881s 876ms/step - loss: 0.0130 - val_word_accuracy: 0.0285 - val_loss: 0.0117\n",
      "Epoch 15/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0122\n",
      "Epoch 00015: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0122 - val_word_accuracy: 0.0302 - val_loss: 0.0122\n",
      "Epoch 16/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0153\n",
      "Epoch 00016: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 881s 877ms/step - loss: 0.0153 - val_word_accuracy: 0.0302 - val_loss: 0.0144\n",
      "Epoch 17/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0119\n",
      "Epoch 00017: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 881s 877ms/step - loss: 0.0119 - val_word_accuracy: 0.0302 - val_loss: 0.0116\n",
      "Epoch 18/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0115\n",
      "Epoch 00018: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 880s 876ms/step - loss: 0.0115 - val_word_accuracy: 0.0302 - val_loss: 0.0116\n",
      "Epoch 19/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0098\n",
      "Epoch 00019: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0098 - val_word_accuracy: 0.0268 - val_loss: 0.0130\n",
      "Epoch 20/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0099\n",
      "Epoch 00020: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 874ms/step - loss: 0.0099 - val_word_accuracy: 0.0302 - val_loss: 0.0101\n",
      "Epoch 21/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0089\n",
      "Epoch 00021: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0089 - val_word_accuracy: 0.0302 - val_loss: 0.0107\n",
      "Epoch 22/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0081\n",
      "Epoch 00022: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 878s 874ms/step - loss: 0.0081 - val_word_accuracy: 0.0302 - val_loss: 0.0131\n",
      "Epoch 23/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0069\n",
      "Epoch 00023: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 878s 874ms/step - loss: 0.0069 - val_word_accuracy: 0.0302 - val_loss: 0.0096\n",
      "Epoch 24/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00024: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 878s 874ms/step - loss: 0.0095 - val_word_accuracy: 0.0302 - val_loss: 0.0128\n",
      "Epoch 25/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0088\n",
      "Epoch 00025: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0088 - val_word_accuracy: 0.0302 - val_loss: 0.0105\n",
      "Epoch 26/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0073\n",
      "Epoch 00026: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 878s 873ms/step - loss: 0.0073 - val_word_accuracy: 0.0302 - val_loss: 0.0087\n",
      "Epoch 27/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0076\n",
      "Epoch 00027: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 878s 874ms/step - loss: 0.0076 - val_word_accuracy: 0.0302 - val_loss: 0.0097\n",
      "Epoch 28/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0095\n",
      "Epoch 00028: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 877s 873ms/step - loss: 0.0095 - val_word_accuracy: 0.0201 - val_loss: 0.0138\n",
      "Epoch 29/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0091\n",
      "Epoch 00029: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0091 - val_word_accuracy: 0.0201 - val_loss: 0.0109\n",
      "Epoch 30/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0072\n",
      "Epoch 00030: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 877s 872ms/step - loss: 0.0072 - val_word_accuracy: 0.0251 - val_loss: 0.0160\n",
      "Epoch 31/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0067\n",
      "Epoch 00031: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 877s 873ms/step - loss: 0.0067 - val_word_accuracy: 0.0302 - val_loss: 0.0074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0059\n",
      "Epoch 00032: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 876s 872ms/step - loss: 0.0059 - val_word_accuracy: 0.0285 - val_loss: 0.0078\n",
      "Epoch 33/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0034\n",
      "Epoch 00033: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 878s 873ms/step - loss: 0.0034 - val_word_accuracy: 0.0302 - val_loss: 0.0073\n",
      "Epoch 34/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0035\n",
      "Epoch 00034: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 877s 873ms/step - loss: 0.0035 - val_word_accuracy: 0.0302 - val_loss: 0.0084\n",
      "Epoch 35/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0024\n",
      "Epoch 00035: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 877s 872ms/step - loss: 0.0024 - val_word_accuracy: 0.0302 - val_loss: 0.0058\n",
      "Epoch 36/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00036: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 877s 872ms/step - loss: 0.0028 - val_word_accuracy: 0.0302 - val_loss: 0.0058\n",
      "Epoch 37/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0091\n",
      "Epoch 00037: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 878s 874ms/step - loss: 0.0091 - val_word_accuracy: 0.0251 - val_loss: 0.0106\n",
      "Epoch 38/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00038: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 884s 880ms/step - loss: 0.0043 - val_word_accuracy: 0.0302 - val_loss: 0.0069\n",
      "Epoch 39/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0028\n",
      "Epoch 00039: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 876s 872ms/step - loss: 0.0028 - val_word_accuracy: 0.0302 - val_loss: 0.0062\n",
      "Epoch 40/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0029\n",
      "Epoch 00040: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 877s 873ms/step - loss: 0.0029 - val_word_accuracy: 0.0302 - val_loss: 0.0068\n",
      "Epoch 41/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0033\n",
      "Epoch 00041: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 874ms/step - loss: 0.0033 - val_word_accuracy: 0.0302 - val_loss: 0.0060\n",
      "Epoch 42/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0040\n",
      "Epoch 00042: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 876s 872ms/step - loss: 0.0040 - val_word_accuracy: 0.0302 - val_loss: 0.0053\n",
      "Epoch 43/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00043: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 877s 873ms/step - loss: 0.0021 - val_word_accuracy: 0.0302 - val_loss: 0.0062\n",
      "Epoch 44/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0019\n",
      "Epoch 00044: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0019 - val_word_accuracy: 0.0302 - val_loss: 0.0054\n",
      "Epoch 45/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00045: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 880s 875ms/step - loss: 0.0022 - val_word_accuracy: 0.0302 - val_loss: 0.0052\n",
      "Epoch 46/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00046: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0015 - val_word_accuracy: 0.0302 - val_loss: 0.0062\n",
      "Epoch 47/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0022\n",
      "Epoch 00047: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 879s 875ms/step - loss: 0.0022 - val_word_accuracy: 0.0302 - val_loss: 0.0065\n",
      "Epoch 48/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0017\n",
      "Epoch 00048: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 880s 875ms/step - loss: 0.0017 - val_word_accuracy: 0.0302 - val_loss: 0.0051\n",
      "Epoch 49/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00049: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 882s 878ms/step - loss: 0.0015 - val_word_accuracy: 0.0302 - val_loss: 0.0048\n",
      "Epoch 50/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00050: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 884s 879ms/step - loss: 0.0021 - val_word_accuracy: 0.0302 - val_loss: 0.0045\n",
      "Epoch 51/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0015\n",
      "Epoch 00051: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 882s 878ms/step - loss: 0.0015 - val_word_accuracy: 0.0302 - val_loss: 0.0043\n",
      "Epoch 52/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0021\n",
      "Epoch 00052: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 883s 878ms/step - loss: 0.0021 - val_word_accuracy: 0.0302 - val_loss: 0.0043\n",
      "Epoch 53/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0012\n",
      "Epoch 00053: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 881s 877ms/step - loss: 0.0012 - val_word_accuracy: 0.0302 - val_loss: 0.0040\n",
      "Epoch 54/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0012\n",
      "Epoch 00054: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 882s 878ms/step - loss: 0.0012 - val_word_accuracy: 0.0302 - val_loss: 0.0054\n",
      "Epoch 55/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0014\n",
      "Epoch 00055: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 883s 878ms/step - loss: 0.0014 - val_word_accuracy: 0.0302 - val_loss: 0.0047\n",
      "Epoch 56/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0013\n",
      "Epoch 00056: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 880s 875ms/step - loss: 0.0013 - val_word_accuracy: 0.0302 - val_loss: 0.0053\n",
      "Epoch 57/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0016\n",
      "Epoch 00057: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 882s 878ms/step - loss: 0.0016 - val_word_accuracy: 0.0302 - val_loss: 0.0044\n",
      "Epoch 58/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 0.0011\n",
      "Epoch 00058: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 883s 878ms/step - loss: 0.0011 - val_word_accuracy: 0.0302 - val_loss: 0.0043\n",
      "Epoch 59/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 8.5646e-04\n",
      "Epoch 00059: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 884s 879ms/step - loss: 8.5646e-04 - val_word_accuracy: 0.0302 - val_loss: 0.0048\n",
      "Epoch 60/100\n",
      "1005/1005 [==============================] - ETA: 0s - loss: 9.9866e-04\n",
      "Epoch 00060: saving model to training_checkpoint\\cp.ckpt\n",
      "1005/1005 [==============================] - 883s 878ms/step - loss: 9.9866e-04 - val_word_accuracy: 0.0302 - val_loss: 0.0036\n",
      "Epoch 61/100\n",
      " 603/1005 [=================>............] - ETA: 5:36 - loss: 9.7591e-04"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_ds))\n",
    "\n",
    "# The vocabulary to convert predicted indices into characters\n",
    "idx_to_char = vectorizer.get_vocabulary()\n",
    "display_cb = DisplayOutputs(\n",
    "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",
    ")  # set the arguments as per vocabulary index for '<' and '>'\n",
    "\n",
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=2,\n",
    "    num_feed_forward=400,\n",
    "    target_maxlen=max_target_len,\n",
    "    num_layers_enc=4,\n",
    "    num_layers_dec=1,\n",
    "    num_classes=34,\n",
    ")\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    label_smoothing=0.1,\n",
    ")\n",
    "\n",
    "learning_rate = CustomSchedule(\n",
    "    init_lr=0.00001,\n",
    "    lr_after_warmup=0.001,\n",
    "    final_lr=0.00001,\n",
    "    warmup_epochs=15,\n",
    "    decay_epochs=85,\n",
    "    steps_per_epoch=len(ds),\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=5)\n",
    "\n",
    "# Congela as últimas camadas do codificador\n",
    "num_camadas_para_congelar = 2  # Altere o número de camadas que deseja congelar\n",
    "for camada in model.encoder.layers[-num_camadas_para_congelar:]:\n",
    "    camada.trainable = False\n",
    "\n",
    "# Verifica quais camadas estão congeladas\n",
    "for camada in model.encoder.layers:\n",
    "    print(camada.name, camada.trainable)\n",
    "\n",
    "# Crie um objeto de checkpoint\n",
    "checkpoint_path = \"training_checkpoint/cp.ckpt\"\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=1)\n",
    "\n",
    "load_checkpoint(checkpoint, checkpoint_manager)\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         verbose=1)\n",
    "\n",
    "history = model.fit(ds, validation_data=val_ds, callbacks=checkpoint_callback, epochs=100)\n",
    "\n",
    "\n",
    "model.save_weights('pre_ua_weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWXGWnClRHCi"
   },
   "source": [
    "In practice, you should train for around 100 epochs or more.\n",
    "\n",
    "Some of the predicted text at or around epoch 35 may look as follows:\n",
    "```\n",
    "target:     <as they sat in the car, frazier asked oswald where his lunch was>\n",
    "prediction: <as they sat in the car frazier his lunch ware mis lunch was>\n",
    "\n",
    "target:     <under the entry for may one, nineteen sixty,>\n",
    "prediction: <under the introus for may monee, nin the sixty,>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "#tensorboard_model = TensorBoard(logdir='C:\\\\Users\\\\Gracelli\\\\ASR-keras\\\\logs\\\\fit', graph=tf.compat.v1.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar\n",
    "import tensorflow as tf\n",
    "\n",
    "class CalculateWRA(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, val_ds, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
    "    ):\n",
    "        \"\"\"Calculates Word Recognition Accuracy (WRA) after every epoch\n",
    "\n",
    "        Args:\n",
    "            val_ds: Validation dataset containing the keys \"source\" and \"target\"\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.val_ds = val_ds\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        total_count = 0\n",
    "        correct_count = 0\n",
    "\n",
    "        num_batches = tf.data.experimental.cardinality(self.val_ds).numpy()\n",
    "        progbar = Progbar(target=num_batches, stateful_metrics=['correct_count', 'total_count'])\n",
    "\n",
    "        for batch_num, batch in enumerate(self.val_ds):\n",
    "            source = batch[\"source\"]\n",
    "            target = batch[\"target\"].numpy()\n",
    "            bs = tf.shape(source)[0]\n",
    "            preds = model.generate(source, self.target_start_token_idx)\n",
    "            preds = preds.numpy()\n",
    "\n",
    "            for i in range(bs):\n",
    "                target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "                prediction = \"\"\n",
    "                for idx in preds[i, :]:\n",
    "                    prediction += self.idx_to_char[idx]\n",
    "                    if idx == self.target_end_token_idx:\n",
    "                        break\n",
    "\n",
    "                total_count += 1\n",
    "                if target_text.strip() == prediction.strip():\n",
    "                    correct_count += 1\n",
    "\n",
    "\n",
    "            progbar.update(batch_num+1, values=[('correct_count', correct_count), ('total_count', total_count)])\n",
    "\n",
    "        wra = correct_count / total_count\n",
    "        print(f\"\\nWord Recognition Accuracy (WRA) on validation set: {wra * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calculate_wra = CalculateWRA(val_ds=val_ds, idx_to_token=idx_to_char, target_start_token_idx=27)\n",
    "\n",
    "\n",
    "calculate_wra.on_epoch_end(epoch=0)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "transformer_asr",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
