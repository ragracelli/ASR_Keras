{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k3opHLYJRHCR"
   },
   "source": [
    "# Automatic Speech Recognition with Transformer\n",
    "\n",
    "**Author:** [Apoorv Nandan](https://twitter.com/NandanApoorv)<br>\n",
    "**Date created:** 2021/01/13<br>\n",
    "**Last modified:** 2021/01/13<br>\n",
    "**Description:** Training a sequence-to-sequence Transformer for automatic speech recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNu5BNVtRHCW"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "Automatic speech recognition (ASR) consists of transcribing audio speech segments into text.\n",
    "ASR can be treated as a sequence-to-sequence problem, where the\n",
    "audio can be represented as a sequence of feature vectors\n",
    "and the text as a sequence of characters, words, or subword tokens.\n",
    "\n",
    "For this demonstration, we will use the LJSpeech dataset from the\n",
    "[LibriVox](https://librivox.org/) project. It consists of short\n",
    "audio clips of a single speaker reading passages from 7 non-fiction books.\n",
    "Our model will be similar to the original Transformer (both encoder and decoder)\n",
    "as proposed in the paper, \"Attention is All You Need\".\n",
    "\n",
    "\n",
    "**References:**\n",
    "\n",
    "- [Attention is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)\n",
    "- [Very Deep Self-Attention Networks for End-to-End Speech Recognition](https://arxiv.org/abs/1904.13377)\n",
    "- [Speech Transformers](https://ieeexplore.ieee.org/document/8462506)\n",
    "- [LJSpeech Dataset](https://keithito.com/LJ-Speech-Dataset/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "DaN6AecBRHCX",
    "outputId": "63da7ee3-6437-49b4-b800-e20b163ac42b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.optimizers import schedules\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'  # ou qualquer número de 0 a 3\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "import datetime\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import keras\n",
    "from keras import layers\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Lista todos os dispositivos físicos do tipo 'GPU'\n",
    "devices = tf.config.list_physical_devices('GPU')\n",
    "print(len(devices))  # Se o resultado for maior que 0, uma GPU está sendo usada\n",
    "\n",
    "# Verifica se o TensorFlow foi construído com suporte a CUDA\n",
    "print(tf.test.is_built_with_cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjOYcgmpRHCZ"
   },
   "source": [
    "## Define the Transformer Input Layer\n",
    "\n",
    "When processing past target tokens for the decoder, we compute the sum of\n",
    "position embeddings and token embeddings.\n",
    "\n",
    "When processing audio features, we apply convolutional layers to downsample\n",
    "them (via convolution strides) and process local relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Irr1T-O2RHCZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TokenEmbedding(layers.Layer):\n",
    "    def __init__(self, num_vocab=1000, maxlen=100, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.emb = keras.layers.Embedding(num_vocab, num_hid)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=num_hid)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        x = self.emb(x)\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        return x + positions\n",
    "\n",
    "\n",
    "class SpeechFeatureEmbedding(layers.Layer):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv2 = keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "        self.conv3 = keras.layers.Conv1D(\n",
    "            num_hid, 11, strides=2, padding=\"same\", activation=\"relu\"\n",
    "        )\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return self.conv3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ab14XhPRHCa"
   },
   "source": [
    "## Transformer Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SpEtRUFDRHCb"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                #layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                #layers.Dense(embed_dim),\n",
    "                layers.SeparableConv1D(feed_forward_dim, 3, activation='relu', padding='same'),\n",
    "                layers.SeparableConv1D(embed_dim, 3, padding='same'),\n",
    "\n",
    "\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nfsTgZ6vRHCb"
   },
   "source": [
    "## Transformer Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LRksSL9oRHCc"
   },
   "outputs": [],
   "source": [
    "\n",
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.self_att = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        self.enc_att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.self_dropout = layers.Dropout(0.5)\n",
    "        self.enc_dropout = layers.Dropout(0.1)\n",
    "        self.ffn_dropout = layers.Dropout(0.1)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(feed_forward_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
    "        \"\"\"Masks the upper half of the dot product matrix in self attention.\n",
    "\n",
    "        This prevents flow of information from future tokens to current token.\n",
    "        1's in the lower triangle, counting from the lower right corner.\n",
    "        \"\"\"\n",
    "        i = tf.range(n_dest)[:, None]\n",
    "        j = tf.range(n_src)\n",
    "        m = i >= j - n_src + n_dest\n",
    "        mask = tf.cast(m, dtype)\n",
    "        mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "        )\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, enc_out, target):\n",
    "        input_shape = tf.shape(target)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = self.causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "        target_att = self.self_att(target, target, attention_mask=causal_mask)\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
    "        enc_out = self.enc_att(target_norm, enc_out)\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
    "        return ffn_out_norm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d55jG_L0RHCc"
   },
   "source": [
    "## Complete the Transformer model\n",
    "\n",
    "Our model takes audio spectrograms as inputs and predicts a sequence of characters.\n",
    "During training, we give the decoder the target character sequence shifted to the left\n",
    "as input. During inference, the decoder uses its own past predictions to predict the\n",
    "next token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "OgptmFnFRHCd"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid=64,\n",
    "        num_head=2,\n",
    "        num_feed_forward=128,\n",
    "        source_maxlen=100,\n",
    "        target_maxlen=100,\n",
    "        num_layers_enc=5,\n",
    "        num_layers_dec=3,\n",
    "        num_classes=10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.loss_metric = tf.keras.metrics.Mean(name=\"loss\")\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(\n",
    "            num_vocab=num_classes, maxlen=target_maxlen, num_hid=num_hid\n",
    "        )\n",
    "\n",
    "        self.encoder = tf.keras.Sequential(\n",
    "            [self.enc_input]\n",
    "            + [\n",
    "                TransformerEncoder(num_hid, num_head, num_feed_forward)\n",
    "                for _ in range(num_layers_enc)\n",
    "            ]\n",
    "        )\n",
    "         #Comentar esta sequencia para treinar UA\n",
    "        for i in range(num_layers_dec):\n",
    "            setattr(\n",
    "                self,\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "        \n",
    "        ####################### Congela todas as camadas decodificadoras ################################\n",
    "\n",
    "        #for i in range(num_layers_dec):\n",
    "        #    dec_layer = TransformerDecoder(num_hid, num_head, num_feed_forward)\n",
    "        #    dec_layer.trainable = False  # Congelar a camada\n",
    "        #    setattr(self, f\"dec_layer_{i}\", dec_layer)\n",
    "        #    print(dec_layer.name, dec_layer.trainable)\n",
    "            \n",
    "        ########################Congela apenas uma camada conforme o indice #############################\n",
    "        #\n",
    "        # Supondo que tenha 3 camadas de decodificador e queira congelar apenas a segunda camada\n",
    "        '''\n",
    "        for i in range(num_layers_dec):\n",
    "            dec_layer = TransformerDecoder(num_hid, num_head, num_feed_forward)\n",
    "            if i == 1:  # Se for a segunda camada (índice 1)\n",
    "                dec_layer.trainable = False  # Congelar a camada\n",
    "            setattr(self, f\"dec_layer_{i}\", dec_layer)\n",
    "        '''\n",
    "        #################################################################################################\n",
    "\n",
    "        self.classifier = layers.Dense(num_classes)\n",
    "\n",
    "    def decode(self, enc_out, target):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            y = getattr(self, f\"dec_layer_{i}\")(enc_out, y)\n",
    "        return y\n",
    "\n",
    "    def call(self, inputs):\n",
    "        source = inputs[0]\n",
    "        target = inputs[1]\n",
    "        x = self.encoder(source)\n",
    "        y = self.decode(x, target)\n",
    "        return self.classifier(y)\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_metric]\n",
    "    \n",
    "    def train_step(self, batch):\n",
    "        \"\"\"Processes one batch inside model.fit().\"\"\"\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            preds = self([source, dec_input])\n",
    "            one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "            mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "            loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "            loss = loss_object(one_hot, preds, sample_weight=mask)\n",
    "\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def test_step(self, batch):\n",
    "        source = batch[\"source\"]\n",
    "        target = batch[\"target\"]\n",
    "        dec_input = target[:, :-1]\n",
    "        dec_target = target[:, 1:]\n",
    "        preds = self([source, dec_input])\n",
    "        one_hot = tf.one_hot(dec_target, depth=self.num_classes)\n",
    "        mask = tf.math.logical_not(tf.math.equal(dec_target, 0))\n",
    "        loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "        loss = loss_object(one_hot, preds, sample_weight=mask)\n",
    "\n",
    "        self.loss_metric.update_state(loss)\n",
    "        return {\"loss\": self.loss_metric.result()}\n",
    "\n",
    "    def generate(self, source, target_start_token_idx):\n",
    "        \"\"\"Performs inference over one batch of inputs using greedy decoding.\"\"\"\n",
    "        bs = tf.shape(source)[0]\n",
    "        enc = self.encoder(source)\n",
    "        dec_input = tf.ones((bs, 1), dtype=tf.int32) * target_start_token_idx\n",
    "        dec_logits = []\n",
    "        for i in range(self.target_maxlen - 1):\n",
    "            dec_out = self.decode(enc, dec_input)\n",
    "            logits = self.classifier(dec_out)\n",
    "            logits = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "            last_logit = tf.expand_dims(logits[:, -1], axis=-1)\n",
    "            dec_logits.append(last_logit)\n",
    "            dec_input = tf.concat([dec_input, last_logit], axis=-1)\n",
    "        return dec_input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "liuKciSRRHCd"
   },
   "source": [
    "## Download the dataset LJSpeech\n",
    "\n",
    "Note: This requires ~3.6 GB of disk space and\n",
    "takes ~5 minutes for the extraction of files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "au3uVjM1RHCd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "keras.utils.get_file(\n",
    "    os.path.join(os.getcwd(), \"data.tar.gz\"),\n",
    "    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n",
    "    extract=True,\n",
    "    archive_format=\"tar\",\n",
    "    cache_dir=\".\",\n",
    ")\n",
    "'''\n",
    "\n",
    "saveto = \".\\\\datasets\\\\LJSpeech-1.1\"\n",
    "wavs = glob(\"{}\\\\**\\\\*.wav\".format(saveto), recursive=True)\n",
    "#print(wavs)\n",
    "id_to_text = {}\n",
    "with open(os.path.join(saveto, \"metadata.csv\"), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        id = line.strip().split(\"|\")[0]\n",
    "        text = line.strip().split(\"|\")[2]\n",
    "        id_to_text[id] = text\n",
    "\n",
    "\n",
    "def get_data(wavs, id_to_text, maxlen=50):\n",
    "    \"\"\"returns mapping of audio paths and transcription texts\"\"\"\n",
    "    data = []\n",
    "    for w in wavs:\n",
    "        id = w.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        if len(id_to_text[id]) < maxlen:\n",
    "            data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset UA-Speech"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "au3uVjM1RHCd",
    "scrolled": true
   },
   "source": [
    "'''\n",
    "keras.utils.get_file(\n",
    "    os.path.join(os.getcwd(), \"data.tar.gz\"),\n",
    "    \"https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2\",\n",
    "    extract=True,\n",
    "    archive_format=\"tar\",\n",
    "    cache_dir=\".\",\n",
    ")\n",
    "'''\n",
    "\n",
    "#saveto = \".\\\\datasets\\\\LJSpeech-1.1\"\n",
    "saveto = '.\\\\datasets\\\\uaspeech\\\\wavs'\n",
    "wavs = glob(\"{}\\\\**\\\\*.wav\".format(saveto), recursive=True)\n",
    "id_to_text = {}\n",
    "with open(os.path.join(saveto, \".\\\\transcript\\metadata.csv\"), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        id = line.strip().split(\"|\")[0]\n",
    "        text = line.strip().split(\"|\")[1]\n",
    "        id_to_text[id] = text\n",
    "\n",
    "saveto_test = '.\\\\datasets\\\\uaspeech\\\\test'\n",
    "wavs_test = glob(\"{}\\\\**\\\\*.wav\".format(saveto_test), recursive=True)\n",
    "id_to_text_test = {}\n",
    "with open(os.path.join(saveto_test, \".\\\\transcript\\metadata.csv\"), encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        id_test = line.strip().split(\"|\")[0]\n",
    "        text_test = line.strip().split(\"|\")[1]\n",
    "        id_to_text_test[id_test] = text_test        \n",
    "        \n",
    "\n",
    "def get_data(wavs, id_to_text, maxlen=50):\n",
    "    \"\"\"returns mapping of audio paths and transcription texts\"\"\"\n",
    "    data = []\n",
    "    for w in wavs:\n",
    "        id = w.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        if len(id_to_text[id]) < maxlen:\n",
    "            data.append({\"audio\": w, \"text\": id_to_text[id]})\n",
    "    return data\n",
    "\n",
    "def get_data_test(wavs_test, id_to_text_test, maxlen=50):\n",
    "    \"\"\"returns mapping of audio paths and transcription texts\"\"\"\n",
    "    data_test = []\n",
    "    for w in wavs_test:\n",
    "        id_test = w.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "        if len(id_to_text_test[id_test]) < maxlen:\n",
    "            data_test.append({\"audio\": w, \"text\": id_to_text_test[id_test]})\n",
    "    return data_test\n",
    "\n",
    "print(get_data(wavs, id_to_text, maxlen=50)[1:5])\n",
    "print(get_data_test(wavs_test, id_to_text_test, maxlen=50)[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_data(wavs, id_to_text, maxlen=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4eTa9km4RHCe"
   },
   "source": [
    "## Preprocess the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "2mxWQLOARHCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size 34\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class VectorizeChar:\n",
    "    def __init__(self, max_len=50):\n",
    "        self.vocab = (\n",
    "            [\"-\", \"#\", \"<\", \">\"]\n",
    "            + [chr(i + 96) for i in range(1, 27)]\n",
    "            + [\" \", \".\", \",\", \"?\"]\n",
    "        )\n",
    "        self.max_len = max_len\n",
    "        self.char_to_idx = {}\n",
    "        for i, ch in enumerate(self.vocab):\n",
    "            self.char_to_idx[ch] = i\n",
    "\n",
    "    def __call__(self, text):\n",
    "        text = text.lower()\n",
    "        text = text[: self.max_len - 2]\n",
    "        text = \"<\" + text + \">\"\n",
    "        pad_len = self.max_len - len(text)\n",
    "        return [self.char_to_idx.get(ch, 1) for ch in text] + [0] * pad_len\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        return self.vocab\n",
    "\n",
    "\n",
    "max_target_len = 200  # all transcripts in out data are < 200 characters\n",
    "data = get_data(wavs, id_to_text, max_target_len)\n",
    "#data_test = get_data_test(wavs_test, id_to_text_test, max_target_len) para UASPEECH\n",
    "vectorizer = VectorizeChar(max_target_len)\n",
    "vocab_size = len(vectorizer.get_vocabulary())\n",
    "print(\"vocab size\", len(vectorizer.get_vocabulary()))\n",
    "\n",
    "\n",
    "def create_text_ds(data):\n",
    "    texts = [_[\"text\"] for _ in data]\n",
    "    text_ds = [vectorizer(t) for t in texts]\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(text_ds)\n",
    "    return text_ds\n",
    "\n",
    "\n",
    "def path_to_audio(path):\n",
    "    # spectrogram using stft\n",
    "    audio = tf.io.read_file(path)\n",
    "    audio, _ = tf.audio.decode_wav(audio, 1)\n",
    "    audio = tf.squeeze(audio, axis=-1)\n",
    "    stfts = tf.signal.stft(audio, frame_length=200, frame_step=80, fft_length=256)\n",
    "    x = tf.math.pow(tf.abs(stfts), 0.5)\n",
    "    # normalisation\n",
    "    means = tf.math.reduce_mean(x, 1, keepdims=True)\n",
    "    stddevs = tf.math.reduce_std(x, 1, keepdims=True)\n",
    "    x = (x - means) / stddevs\n",
    "    audio_len = tf.shape(x)[0]\n",
    "    # padding to 10 seconds\n",
    "    pad_len = 2754\n",
    "    paddings = tf.constant([[0, pad_len], [0, 0]])\n",
    "    x = tf.pad(x, paddings, \"CONSTANT\")[:pad_len, :]\n",
    "    return x\n",
    "\n",
    "\n",
    "def create_audio_ds(data):\n",
    "    flist = [_[\"audio\"] for _ in data]\n",
    "    audio_ds = tf.data.Dataset.from_tensor_slices(flist)\n",
    "    audio_ds = audio_ds.map(path_to_audio, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    return audio_ds\n",
    "\n",
    "\n",
    "def create_tf_dataset(data, bs=64):\n",
    "    audio_ds = create_audio_ds(data)\n",
    "    text_ds = create_text_ds(data)\n",
    "    ds = tf.data.Dataset.zip((audio_ds, text_ds))\n",
    "    ds = ds.map(lambda x, y: {\"source\": x, \"target\": y})\n",
    "    ds = ds.batch(bs)\n",
    "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(data) * 0.99) # comentar para UA\n",
    "train_data = data[:split] # comentar para UA\n",
    "train_data = data\n",
    "test_data = data[split:] # comentar para UA\n",
    "#test_data = data_test # Para UA\n",
    "ds = create_tf_dataset(train_data, bs=64)\n",
    "val_ds = create_tf_dataset(test_data, bs=4) #orig. = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: {source: (None, None, 129), target: (None, 200)}, types: {source: tf.float32, target: tf.int32}>\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PrefetchDataset shapes: {source: (None, None, 129), target: (None, 200)}, types: {source: tf.float32, target: tf.int32}>\n"
     ]
    }
   ],
   "source": [
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szBaODmsRHCg"
   },
   "source": [
    "## Callbacks to display predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1L-YvvMxRHCg"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DisplayOutputs(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, batch, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
    "    ):\n",
    "        \"\"\"Displays a batch of outputs after every epoch\n",
    "\n",
    "        Args:\n",
    "            batch: A test batch containing the keys \"source\" and \"target\"\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.batch = batch\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 5 != 0:\n",
    "            return\n",
    "        source = self.batch[\"source\"]\n",
    "        target = self.batch[\"target\"].numpy()\n",
    "        bs = tf.shape(source)[0]\n",
    "        preds = self.model.generate(source, self.target_start_token_idx)\n",
    "        preds = preds.numpy()\n",
    "        for i in range(bs):\n",
    "            target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "            prediction = \"\"\n",
    "            for idx in preds[i, :]:\n",
    "                prediction += self.idx_to_char[idx]\n",
    "                if idx == self.target_end_token_idx:\n",
    "                    break\n",
    "            print(f\"target:     {target_text.replace('-','')}\")\n",
    "            print(f\"prediction: {prediction}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6JfOL_oARHCg"
   },
   "source": [
    "## Learning rate schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "D-dRLFkNRHCh"
   },
   "outputs": [],
   "source": [
    "\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        init_lr=0.00001,\n",
    "        lr_after_warmup=0.001,\n",
    "        final_lr=0.00001,\n",
    "        warmup_epochs=15,\n",
    "        decay_epochs=85,\n",
    "        steps_per_epoch=203,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.init_lr = init_lr\n",
    "        self.lr_after_warmup = lr_after_warmup\n",
    "        self.final_lr = final_lr\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.decay_epochs = decay_epochs\n",
    "        self.steps_per_epoch = steps_per_epoch\n",
    "\n",
    "    def calculate_lr(self, epoch):\n",
    "        \"\"\"linear warm up - linear decay\"\"\"\n",
    "        warmup_lr = (\n",
    "            self.init_lr\n",
    "            + ((self.lr_after_warmup - self.init_lr) / (self.warmup_epochs - 1)) * epoch\n",
    "        )\n",
    "        decay_lr = tf.math.maximum(\n",
    "            self.final_lr,\n",
    "            self.lr_after_warmup\n",
    "            - (epoch - self.warmup_epochs)\n",
    "            * (self.lr_after_warmup - self.final_lr)\n",
    "            / self.decay_epochs,\n",
    "        )\n",
    "        return tf.math.minimum(warmup_lr, decay_lr)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        epoch = step // self.steps_per_epoch\n",
    "        epoch = tf.cast(epoch, \"float32\")\n",
    "        return self.calculate_lr(epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ne33ZwMORHCh"
   },
   "source": [
    "## Create & train the end-to-end model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva e carrega o modelo"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def create_callbacks(display_cb, filepath):\n",
    "    # Função para salvar o melhor modelo baseado na perda de validação\n",
    "    model_checkpoint_callback = ModelCheckpoint(\n",
    "        filepath=filepath,\n",
    "        save_weights_only=True,\n",
    "        monitor='val_loss',\n",
    "        mode='min',\n",
    "        save_best_only=True)\n",
    "\n",
    "    # Adicione model_checkpoint_callback à lista de callbacks.\n",
    "    callbacks_list = [display_cb, model_checkpoint_callback]\n",
    "    return callbacks_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(checkpoint, checkpoint_manager):\n",
    "    # Função para retomar o treinamento de onde parou\n",
    "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
    "    if checkpoint_manager.latest_checkpoint:\n",
    "        print(\"Restaurado de {}\".format(checkpoint_manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Inicializando do zero.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bby6svtcRHCh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando do zero.\n",
      "Epoch 1/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.5391target:     <the increased information supplied by other agencies will be wasted.>\n",
      "prediction: <the alithe t t t o ar t at the t t the t io s the t t the te te the e the t t t othe te the at thee t te t te ten the an oe o the the e the the the e the te the the the the t a the the the te te t t \n",
      "\n",
      "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
      "prediction: <the a t the the t t the at the o t the t io s the t t the te the t are t t t in othe the the t thee t te t te ten the the an the te t e the the the o the the the ore t te t t a the the the te te t t \n",
      "\n",
      "target:     <its present manual filing system is obsolete#>\n",
      "prediction: <the alithe t t t o ar t at the t t the t io s the t t the te te the e the t t t othe te the at thee t te t te ten the an oe o the the e the the the e the te the the the the t a the the the te te t t \n",
      "\n",
      "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
      "prediction: <the a t the the t t the at the o t the t io s the t t the te the t are t t t in othe the the t thee t te t te ten the the an the te t e the the the o t t the the ore t te t t a the the the te te t t \n",
      "\n",
      "\n",
      "Epoch 00001: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 136s 609ms/step - loss: 1.5391 - val_loss: 1.4841\n",
      "Epoch 2/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.2508\n",
      "Epoch 00002: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 117s 568ms/step - loss: 1.2508 - val_loss: 1.2777\n",
      "Epoch 3/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.1862\n",
      "Epoch 00003: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 117s 569ms/step - loss: 1.1862 - val_loss: 1.2195\n",
      "Epoch 4/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.1032\n",
      "Epoch 00004: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 117s 569ms/step - loss: 1.1032 - val_loss: 1.0876\n",
      "Epoch 5/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 1.0181\n",
      "Epoch 00005: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 118s 572ms/step - loss: 1.0181 - val_loss: 0.9939\n",
      "Epoch 6/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.9533target:     <the increased information supplied by other agencies will be wasted.>\n",
      "prediction: <the secres and the secres and and and and and and and and the president the secret>\n",
      "\n",
      "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
      "prediction: <the secres as and and the secre and and and and and and and and the secre and and the secres and and and and the seconsident of the president the presece.>\n",
      "\n",
      "target:     <its present manual filing system is obsolete#>\n",
      "prediction: <the secres of the secres asssinal and and the secres>\n",
      "\n",
      "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
      "prediction: <the secres of the sevence of the secre and and and and and and and and and the secre and and and and and and and the secres of the proulde.>\n",
      "\n",
      "\n",
      "Epoch 00006: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 125s 610ms/step - loss: 0.9533 - val_loss: 0.9084\n",
      "Epoch 7/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.8913\n",
      "Epoch 00007: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 117s 570ms/step - loss: 0.8913 - val_loss: 0.8083\n",
      "Epoch 8/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.8081\n",
      "Epoch 00008: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 119s 580ms/step - loss: 0.8081 - val_loss: 0.6880\n",
      "Epoch 9/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.6648\n",
      "Epoch 00009: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 118s 574ms/step - loss: 0.6648 - val_loss: 0.4655\n",
      "Epoch 10/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.4641\n",
      "Epoch 00010: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 119s 578ms/step - loss: 0.4641 - val_loss: 0.2900\n",
      "Epoch 11/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.3339target:     <the increased information supplied by other agencies will be wasted.>\n",
      "prediction: <the incromation supplied by other agencies will be wasted.>\n",
      "\n",
      "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
      "prediction: <present than the present you gress must develop the capasited basite the vellop the could as must than the present jubects on amore suffice becrative been.>\n",
      "\n",
      "target:     <its present manual filing system is obsolete#>\n",
      "prediction: <its present mannial the final the finaling sistem is obseley t.>\n",
      "\n",
      "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
      "prediction: <it maxines were altments world, and inother officent nother governmen officent abolowment exas no and in other government officence world whide leas.>\n",
      "\n",
      "\n",
      "Epoch 00011: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 127s 619ms/step - loss: 0.3339 - val_loss: 0.1995\n",
      "Epoch 12/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2633\n",
      "Epoch 00012: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 117s 571ms/step - loss: 0.2633 - val_loss: 0.1473\n",
      "Epoch 13/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.2188\n",
      "Epoch 00013: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 118s 572ms/step - loss: 0.2188 - val_loss: 0.1189\n",
      "Epoch 14/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.1913\n",
      "Epoch 00014: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 117s 571ms/step - loss: 0.1913 - val_loss: 0.1019\n",
      "Epoch 15/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.1713\n",
      "Epoch 00015: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 120s 585ms/step - loss: 0.1713 - val_loss: 0.0865\n",
      "Epoch 16/200\n",
      "205/205 [==============================] - ETA: 0s - loss: 0.1511target:     <the increased information supplied by other agencies will be wasted.>\n",
      "prediction: <the increased information supplied by other agencies will be wasted.>\n",
      "\n",
      "target:     <prs must develop the capacity to classify its subjects on a more sophisticated basis than the present geographic breakdown.>\n",
      "prediction: <prs must develop the copasity to classify its on a more suffice smust than the present jubelop the couph the couph the couph the coto classificie agraphic belop the cupasitie t t to thevemeam>\n",
      "\n",
      "target:     <its present manual filing system is obsolete#>\n",
      "prediction: <its present manual filey system is obsoly t.>\n",
      "\n",
      "target:     <it makes no use of the recent developments in automatic data processing which are widely used in the business world and in other government offices.>\n",
      "prediction: <it makes no used in the business were old an other government officessing which of the recent developments in the bused in the bused in the business.>\n",
      "\n",
      "\n",
      "Epoch 00016: saving model to training_checkpoint\\cp.ckpt\n",
      "205/205 [==============================] - 132s 642ms/step - loss: 0.1511 - val_loss: 0.0718\n",
      "Epoch 17/200\n",
      " 64/205 [========>.....................] - ETA: 1:19 - loss: 0.1470"
     ]
    }
   ],
   "source": [
    "batch = next(iter(val_ds))\n",
    "\n",
    "# The vocabulary to convert predicted indices into characters\n",
    "idx_to_char = vectorizer.get_vocabulary()\n",
    "display_cb = DisplayOutputs(\n",
    "    batch, idx_to_char, target_start_token_idx=2, target_end_token_idx=3\n",
    ")  # set the arguments as per vocabulary index for '<' and '>'\n",
    "\n",
    "model = Transformer(\n",
    "    num_hid=200,\n",
    "    num_head=2,\n",
    "    num_feed_forward=400,\n",
    "    target_maxlen=max_target_len,\n",
    "    num_layers_enc=5,\n",
    "    num_layers_dec=3,\n",
    "    num_classes=vocab_size, #original = num_classes=34,\n",
    ")\n",
    "loss_fn = tf.keras.losses.CategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    label_smoothing=0.1,\n",
    ")\n",
    "\n",
    "learning_rate = CustomSchedule(\n",
    "    init_lr=0.00001,\n",
    "    lr_after_warmup=0.001,\n",
    "    final_lr=0.00001,\n",
    "    warmup_epochs=15,\n",
    "    decay_epochs=85,\n",
    "    steps_per_epoch=len(ds),\n",
    ")\n",
    "\n",
    "#carregar o modelo pre-treinado\n",
    "#model.load_weights('pre_lj_pesos.h5')\n",
    "#model.load_weights('pre_ua_pesos.h5')\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn)\n",
    "\n",
    "# Configure o callback do TensorBoard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = TensorBoard(log_dir=log_dir, histogram_freq=5)\n",
    "\n",
    "'''\n",
    "# Congela as últimas camadas do codificador\n",
    "num_camadas_para_congelar = 2  # Altere o número de camadas que deseja congelar\n",
    "for camada in model.encoder.layers[-num_camadas_para_congelar:]:\n",
    "    camada.trainable = False\n",
    "\n",
    "# Verifique quais camadas estão congeladas\n",
    "for camada in model.encoder.layers:\n",
    "    print(camada.name, camada.trainable)\n",
    "'''\n",
    "# Crie um objeto de checkpoint\n",
    "checkpoint_path = \"training_checkpoint/cp.ckpt\"\n",
    "checkpoint = tf.train.Checkpoint(model=model)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_path, max_to_keep=3)\n",
    "\n",
    "# Carregue o ponto de verificação, se existir\n",
    "load_checkpoint(checkpoint, checkpoint_manager)\n",
    "\n",
    "# Crie um objeto de callback para salvar o modelo\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         verbose=1)\n",
    "\n",
    "history = model.fit(ds, validation_data=val_ds, callbacks=[display_cb, checkpoint_callback], epochs=200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salve o modelo final\n",
    "model.save_weights('pre_lj_pesos.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save('ua_modelo', save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWXGWnClRHCi"
   },
   "source": [
    "In practice, you should train for around 100 epochs or more.\n",
    "\n",
    "Some of the predicted text at or around epoch 35 may look as follows:\n",
    "```\n",
    "target:     <as they sat in the car, frazier asked oswald where his lunch was>\n",
    "prediction: <as they sat in the car frazier his lunch ware mis lunch was>\n",
    "\n",
    "target:     <under the entry for may one, nineteen sixty,>\n",
    "prediction: <under the introus for may monee, nin the sixty,>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "#tensorboard_model = TensorBoard(logdir='/path/to/log', graph=tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotina de testes"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from tensorflow.keras.utils import Progbar\n",
    "import tensorflow as tf\n",
    "\n",
    "class CalculateWRA(keras.callbacks.Callback):\n",
    "    def __init__(\n",
    "        self, val_ds, idx_to_token, target_start_token_idx=27, target_end_token_idx=28\n",
    "    ):\n",
    "        \"\"\"Calculates Word Recognition Accuracy (WRA) after every epoch\n",
    "\n",
    "        Args:\n",
    "            val_ds: Validation dataset containing the keys \"source\" and \"target\"\n",
    "            idx_to_token: A List containing the vocabulary tokens corresponding to their indices\n",
    "            target_start_token_idx: A start token index in the target vocabulary\n",
    "            target_end_token_idx: An end token index in the target vocabulary\n",
    "        \"\"\"\n",
    "        self.val_ds = val_ds\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        total_count = 0\n",
    "        correct_count = 0\n",
    "\n",
    "        num_batches = tf.data.experimental.cardinality(self.val_ds).numpy()\n",
    "        progbar = Progbar(target=num_batches, stateful_metrics=['correct_count', 'total_count'])\n",
    "\n",
    "        for batch_num, batch in enumerate(self.val_ds):\n",
    "            source = batch[\"source\"]\n",
    "            target = batch[\"target\"].numpy()\n",
    "            bs = tf.shape(source)[0]\n",
    "            preds = model.generate(source, self.target_start_token_idx)\n",
    "            preds = preds.numpy()\n",
    "\n",
    "            for i in range(bs):\n",
    "                target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "                prediction = \"\"\n",
    "                for idx in preds[i, :]:\n",
    "                    prediction += self.idx_to_char[idx]\n",
    "                    if idx == self.target_end_token_idx:\n",
    "                        break\n",
    "\n",
    "                total_count += 1\n",
    "                if target_text.strip() == prediction.strip():\n",
    "                    correct_count += 1\n",
    "\n",
    "            # Atualizando a barra de progresso\n",
    "            progbar.update(batch_num+1, values=[('correct_count', correct_count), ('total_count', total_count)])\n",
    "\n",
    "        wra = correct_count / total_count\n",
    "        print(f\"\\nWord Recognition Accuracy (WRA) on validation set: {wra * 100}%\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Instanciando o callback\n",
    "calculate_wra = CalculateWRA(val_ds=val_ds, idx_to_token=idx_to_char, target_start_token_idx=27)\n",
    "\n",
    "# Chamar o método on_epoch_end do callback para fazer as predições e calcular o WRA\n",
    "calculate_wra.on_epoch_end(epoch=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('pre_lj_pesos.h5')\n",
    "class EvaluateModel(keras.callbacks.Callback):\n",
    "    def __init__(self, val_ds, idx_to_token, target_start_token_idx=27, target_end_token_idx=28):\n",
    "        self.val_ds = val_ds\n",
    "        self.target_start_token_idx = target_start_token_idx\n",
    "        self.target_end_token_idx = target_end_token_idx\n",
    "        self.idx_to_char = idx_to_token\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        for i, batch in enumerate(self.val_ds):\n",
    "            source = batch[\"source\"]\n",
    "            target = batch[\"target\"].numpy()\n",
    "            bs = tf.shape(source)[0]\n",
    "            preds = model.generate(source, self.target_start_token_idx)\n",
    "            preds = preds.numpy()\n",
    "            for i in range(bs):\n",
    "                target_text = \"\".join([self.idx_to_char[_] for _ in target[i, :]])\n",
    "                prediction = \"\"\n",
    "                for idx in preds[i, :]:\n",
    "                    prediction += self.idx_to_char[idx]\n",
    "                    if idx == self.target_end_token_idx:\n",
    "                        break\n",
    "                print(f\"target:     {target_text.replace('-','')}\")\n",
    "                print(f\"prediction: {prediction}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = EvaluateModel(val_ds=val_ds, idx_to_token=idx_to_char)\n",
    "callback.model = model  # assuming `model` is your trained model\n",
    "callback.on_train_end()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "transformer_asr",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
